## 1. Problem
Emotion recognition has been increasingly playing an essential role
in human-computer interactions. Unimodal emotion classifiers, systems
that rely on a single aspect of emotional expression, have not shown
outstanding performance. To address this, multi-modal recognizers,
which are trained on multiple aspects of emotional expression, have
been developed. Another issue is that data collected for training
is lab-made and usually spoken by a single person, which means emotional intensity and arousal could be
misleading and not reflecting real-world human interactions.

As we have several sources of data, the way we combine them could affect
the performance of recognition system. which feature classes should be kept,
which classes should be ignored. Should we combine all feature classes then
do classification or do prediction on each feature class then combine
classification result into the final output? Those questions could be
partially answered in the following sections.

## 2. Data
[MELD](https://github.com/declare-lab/MELD/tree/master?tab=readme-ov-file) contains
1400 dialogues and 13000 utterances from Friends TV series. All dialogues has multiple speakers involved and simulate daily conversations. Although these
conversations were scripted in advance and acted in a controlled environment,
the authenticity is sufficiently high as speakers are professional actors/actresses.

There are 7 emotions: Anger, Disgust, Sadness, Joy, Neutral, Surprise, and
Fear, which are labeled to each utterance in a dialogue. Additionally, the
dataset has 3 sentiment, positive, negative, and neutral, annotated to each
utterance.

## 3. Methods and Experiments
### 3.1. Methods
A tri-modal recognition system which combine three data classes:
audio, text, and video, is utilized in this project. In addition, the way we combine these three modals should
not only preserve the characteristics of each modal but also reflect their
inter-modal relationship. Hence, the hybrid fusion model is selected in this
project. The hybrid fusion model firstly combines extracted features from two
classes then do classification on the newly-merged features. Given that bimodal
classification result, the prediction result from the third feature class is
combined to output the final emotion recognition result.

To evaluate how good our system is, we will compare the F-Score obtained for
each emotion and sentiment to the result generated by the baseline model of
MELD data set. The baseline models have 3 several variants: unimodal of audio
, unimodal of text, bimodal of text and audio. 

### 3.2. Experiments



## 4. Division of Work
- Cuong: implement tri-modal emotion recognition system
- Daniel: TODO!
- Raihan: TODO!

### My suggestion for groupmates
I have two other major tasks:
- Bi-modal implementation:
  - Daniel/Raihan: audio and video
  - Daniel/Raihan: text and video
  - Daniel/Raihan: improved version of audio and text.
- Unimodal implementation:
  - Daniel/Raihan: improved version of audio.
  - Daniel/Raihan: improved version of text.

**NOTE**: improved version means that your model should have one or many following
upsides compared to the baseline model:
- use less parameters
- less computation
- higher accuracy

To achieve those advantages you could re-extract features from raw data to find
more highly-related to the emotion by using a more advanced, up-to-date model
(as I see that some models authors used have been out-of-dated).
For example, audio features from baseline MELD were extracted using the openSMILE
model. However, I have found newer model to do this task called [Wave2Vec](https://huggingface.co/docs/transformers/model_doc/wav2vec2). Try to be creative :)).

Feel free to suggest tasks you wanna do, I would love to hear ideas from you guys.

## 5. Notes for group members
- Baseline mode code: https://github.com/declare-lab/MELD/tree/master/baseline
- Look at this [MELD paper](https://arxiv.org/pdf/1810.02508), **Experiments** section,to get the idea of how features were extracted and selection of baseline models.
- Look at this [A review of Multi-modal Emotion Recognition Systems](https://www.sciencedirect.com/science/article/pii/S092523122300989X?via%3Dihub), **Classifier** section, to have an overview of fusion model combinations. Moreover, you could see a pool of machine learning models used efficiently in a wild there, so that if you do not know which model you should start with, please go to that paper.
